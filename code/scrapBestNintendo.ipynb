{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ef4f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "_ - _ SCRAPPING _ - _\n",
      "\n",
      "\n",
      "Scrap en cours :\n",
      " 100% terminé !                    \n",
      "\n",
      "5464 données collectées. \n",
      "\n",
      "_ - _ DATAFRAME _ - _\n",
      "\n",
      "\n",
      "Check avant export :\n",
      "561  doublons présents dans le DataFrame.\n",
      "Tous les doublons ont été supprimés !\n",
      "Il n'y a aucune données manquante dans le DataFrame.\n",
      "\n",
      "\n",
      "Export en cours :\n",
      "Export réussi sous : nintendo_bestsellers_scrapping.csv\n"
     ]
    }
   ],
   "source": [
    "# (_.~\" PRESENTATION \"~._) \n",
    "\n",
    "'''\n",
    "Ce code à pour but  de scrapper les meilleurs ventes sur le site officiel\n",
    "de Nintendo. Il s'organise comme ceci :\n",
    "\n",
    "- Import des librairies\n",
    "- Déclarations (méthodes et variables)\n",
    "- Code principal (scrapping)\n",
    "- Nettoyage et export du DataFrame\n",
    "\n",
    "'''\n",
    "\n",
    "# (_.~\" IMPORTS \"~._) \n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ---\n",
    "\n",
    "# (_.~\" METHODES \"~._)  \n",
    "\n",
    "# NETTOYAGE SIMPLE DES BALISES\n",
    "\n",
    "def simpleClean(toClean) :\n",
    "    '''Cette méthode sert à retirer les balises lors du scrapping \n",
    "    et peut servir (par exemple) pour les titres.\n",
    "    \n",
    "    Entrée type list :\n",
    "    La méthode prend l'argument toClean.\n",
    "    \n",
    "    Fonctionnement :\n",
    "    On applique le filtre to_clean grace au re.compile('<.*?>'). \n",
    "    Il est donc nécessaire d'importer \"re\" en amont de la méthode.\n",
    "    \n",
    "    Sortie type list :\n",
    "    La méthode retourne la liste finalClean instanciée au début.\n",
    "    '''\n",
    "    # Instance de finalClean\n",
    "    finalClean = []\n",
    "    \n",
    "    # Nettoyage de chaque entitée de la liste\n",
    "    for thingToClean in toClean :\n",
    "        to_clean = re.compile('<.*?>')\n",
    "        toCleanStr = str(thingToClean)\n",
    "        toCleanStr = re.sub(to_clean, '',toCleanStr)\n",
    "        finalClean.append(toCleanStr)\n",
    "    \n",
    "    # Remplacement de l'ancienne liste par la liste clean\n",
    "    return finalClean\n",
    "\n",
    "# NETTOYAGE AVANCE POUR LES VENTES\n",
    "\n",
    "def salesClean(toClean) :\n",
    "    '''Cette méthode sert à retirer les balises lors du scrapping \n",
    "    et séléctionner uniquement les ventes réalisées.\n",
    "    \n",
    "    Entrée type list :\n",
    "    La méthode prend l'argument toClean.\n",
    "    \n",
    "    Fonctionnement :\n",
    "    On applique le filtre to_clean grace au re.compile('<.*?>'). \n",
    "    Il est donc nécessaire d'importer \"re\" en amont de la méthode.\n",
    "    Il retire ensuite la fin de l'élément correspondant à :\n",
    "    \" million pcs.\" pour finalement le convertir en float.\n",
    "    \n",
    "    Sortie type list :\n",
    "    La méthode retourne la liste finalClean instanciée au début.\n",
    "    '''\n",
    "    # Instance de finalClean\n",
    "    finalClean = []\n",
    "    \n",
    "    # Nettoyage de chaque entitée de la liste et conversion\n",
    "    for salesToClean in toClean :\n",
    "        to_clean = re.compile('<.*?>')\n",
    "        toCleanStr = str(salesToClean)\n",
    "        toCleanStr = re.sub(to_clean, '',toCleanStr)\n",
    "        finalClean.append(float(toCleanStr[:-13]))\n",
    "    \n",
    "    # Remplacement de l'ancienne liste par la liste clean\n",
    "    return finalClean\n",
    "\n",
    "# ---\n",
    "\n",
    "# (_.~\" DECLARATIONS VARIABLES \"~._) \n",
    "\n",
    "'''\n",
    "\n",
    "CONTEXTE :\n",
    "\n",
    "Après observation, on peut voir que les chiffres de Nintendo \n",
    "sont modifiés fin mars, fin juin, fin septembre et fin décembre.\n",
    "Notre étude se concentrera sur les dates de mars 2017 à fin 2022.\n",
    "Nous allons définir les URLs de chaque trimestre.\n",
    "\n",
    "On utilisera web.archive.org que l'on peut dissequer ainsi :\n",
    "https://web.archive.org/web/ <= site utilisé pour retrouver les archives\n",
    "20190923080858 <= date de l'archive au format YYYYMMDDHHHHHH\n",
    "/https://www.nintendo.co.jp/ir/en/finance/software/ <= site concerné\n",
    "index <= Console concernée. Ici \"index\" == Switch\n",
    ".html <= fin de l'URL\n",
    "\n",
    "NOTE : Il faudra faire attention à supprimer les lignes créées en Fev 2017\n",
    "       dans le df, effectivement elle sera doublée car la Switch viens \n",
    "       juste d'être sortie\n",
    "       \n",
    "'''\n",
    "\n",
    "# Instance du DataFrame principal\n",
    "\n",
    "df = pd.DataFrame(columns=['Date','Hardware','Game','Sales'])\n",
    "\n",
    "# Instance des listes pour le scrapping\n",
    "\n",
    "hardwareList = [\"index\",\"wiiu\",\"3ds\",\"wii\",\"ds\"]\n",
    "yearList = [\"2017\",\"2018\",\"2019\",\"2020\",\"2021\",\"2022\",\"2023\"]\n",
    "monthList = [\"02\",\"05\",\"08\",\"11\"]\n",
    "\n",
    "# Déclaration des variables nécessaires aux URLs où la place 02 et 04\n",
    "# sont réservés pour les dates et les hardwares\n",
    "\n",
    "url01 = \"https://web.archive.org/web/\"\n",
    "url03 = \"15120000/https://www.nintendo.co.jp/ir/en/finance/software/\"\n",
    "url05 = \".html\"\n",
    "\n",
    "# Variables nécessaire à la création d'URL\n",
    "\n",
    "urlList = []\n",
    "url = \"\"\n",
    "\n",
    "# Variable pour connaitre le total de page scrappé/restante\n",
    "actualPage = 0\n",
    "totalPage = len(hardwareList)*len(yearList)*len(monthList)\n",
    "\n",
    "# ---\n",
    "\n",
    "# (_.~\" CODE \"~._)  \n",
    "\n",
    "'''\n",
    "Le code boucle parmis les hardwares, les années puis les mois (déclarés plus haut). \n",
    "Chaque page créée sera scrappé puis les données seront ajoutées au DataFrame df.\n",
    "\n",
    "Note : la date du DataFrame correspond à la mise à jour indiquée par Nintendo et pas\n",
    "à la date de l'archive scrappé depuis e site \"web.archive.org\" .\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"_ - _ SCRAPPING _ - _\")\n",
    "print(\"\\n\")\n",
    "print(\"Scrap en cours :\")\n",
    "print(\"00 % Starting ...\", end=\"\\r\")\n",
    "\n",
    "for hardware in hardwareList:\n",
    "    \n",
    "    for year in yearList:\n",
    "        \n",
    "        for month in monthList:\n",
    "            url = url01 + year + month + url03 + hardware + url05\n",
    "            \n",
    "            # Changement de hardwareTemp en \"Switch\" si \"index\"\n",
    "            if hardware == \"index\":\n",
    "                hardwareTemp = \"switch\"\n",
    "            else:\n",
    "                hardwareTemp = hardware\n",
    "            \n",
    "            # Pour savoir où on se situe en scrapping\n",
    "            actualPage += 1\n",
    "            actualPercent = int(actualPage/totalPage*100)\n",
    "            print(\"\\r\",actualPercent,\"% effectués (\", hardwareTemp, year, month,\")            \", end=\"\\r\")\n",
    "            \n",
    "            \n",
    "            # (_.~\" SCRAPPIIIIING TIME \"~._)\n",
    "   \n",
    "            # Récupération et stockage du code HTML\n",
    "            page = urlopen(url)\n",
    "            soup = bs(page, \"html.parser\")\n",
    "            \n",
    "            # ---\n",
    "    \n",
    "            # oOoOoOo TEXT MINING DES DONNEES oOoOoOo\n",
    "    \n",
    "            # Récupération de la date de mis à Jour des données selon Nintendo\n",
    "            date = soup.find('div', attrs = {'class' : 'sinf_date'}).string\n",
    "    \n",
    "            # Recupération de l'année et du mois de la mise à jour\n",
    "            dateUpdateStr = date[8:-2]\n",
    "            dateUpdate = datetime.strptime(dateUpdateStr, '%B %d, %Y')\n",
    "\n",
    "            # Instance des listes contenant les titres et les ventes\n",
    "            titles = []\n",
    "            sales = []\n",
    "\n",
    "            # Récupération des données\n",
    "            titles = soup.findAll('p', attrs = {'class' : \"sales_title\"})\n",
    "            sales = soup.findAll('p', attrs = {'class' : \"sales_value\"})\n",
    "            # Nettoyage des données\n",
    "            titles = simpleClean(titles)\n",
    "            sales = salesClean(sales)\n",
    "            \n",
    "            # ---\n",
    "    \n",
    "            # oOoOoOo CREATION DU DATAFRAME oOoOoOo\n",
    "            \n",
    "            # Création des données à insérer dans les colonnes Date et Console\n",
    "            tempDateList = [dateUpdate for i in range(len(titles))]\n",
    "            tempHardwareList = [hardwareTemp for i in range(len(titles))]\n",
    "    \n",
    "            # Création du DataFrame contenant les lignes à ajouter\n",
    "            dfSoup = pd.DataFrame(list(zip(tempDateList, tempHardwareList, titles, sales)), \n",
    "                          columns=['Date','Hardware','Game','Sales'])\n",
    "\n",
    "            # Fusion des deux DataFrame\n",
    "            df = pd.concat([df, dfSoup])\n",
    "            \n",
    "\n",
    "# ---\n",
    "\n",
    "print(\"\\r 100% terminé !                    \", end=\"\\r\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Total des données\n",
    "xdf, ydf = df.shape\n",
    "totalData = xdf * ydf\n",
    "\n",
    "print(totalData, \"données collectées. \\n\")\n",
    "\n",
    "# (_.~\" NETTOYAGE PRIMAIRE DU DATAFRAME \"~._) \n",
    "\n",
    "print(\"_ - _ DATAFRAME _ - _\")\n",
    "print(\"\\n\")\n",
    "print(\"Check avant export :\")\n",
    "\n",
    "# Reset de l'index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Doublons check avant suppression\n",
    "doublons = df.duplicated().sum()\n",
    "if doublons > 0:\n",
    "    print(doublons,\" doublons présents dans le DataFrame.\")\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    doublons = df.duplicated().sum()\n",
    "    if doublons > 0:\n",
    "        print(doublons,\" doublons restent après suppression dans le DataFrame.\")\n",
    "    else:\n",
    "        print(\"Tous les doublons ont été supprimés !\")\n",
    "else:\n",
    "    print(\"Il n'y a aucun doublon dans le DataFrame.\")\n",
    "    \n",
    "# NaN check\n",
    "nanDfTotal = df.isna().sum().sum()\n",
    "nanDf = df.isna().sum()\n",
    "if nanDfTotal > 0:\n",
    "    print(\"Données manquantes dans le DataFrame\")\n",
    "    print(nanDf)\n",
    "else:\n",
    "    print(\"Il n'y a aucune données manquante dans le DataFrame.\")\n",
    "    \n",
    "# ---\n",
    "\n",
    "# (_.~\" EXPORT \"~._) \n",
    "\n",
    "# Export du Dataframe dans un fichier CSV\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Export en cours :\")\n",
    "\n",
    "csvName = 'nintendo_bestsellers_scrapping.csv'\n",
    "\n",
    "df.to_csv(csvName, index=False)\n",
    "print(\"Export réussi sous :\", csvName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
