{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec8b521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "_ - _ SCRAPPING _ - _\n",
      "\n",
      "\n",
      "Scrap en cours :\n",
      " 100 % effectués ( 2023 11 )\n",
      " C'est terminé !\n",
      "\n",
      "324 données collectées. \n",
      "\n",
      "_ - _ DATAFRAME _ - _\n",
      "\n",
      "\n",
      "Check avant export :\n",
      "9  doublons présents dans le DataFrame.\n",
      "Tous les doublons ont été supprimés !\n",
      "Il n'y a aucune données manquante dans le DataFrame.\n",
      "\n",
      "\n",
      "Export en cours :\n",
      "Export réussi sous : nintendo_hardware_sales.csv\n"
     ]
    }
   ],
   "source": [
    "# (_.~\" IMPORTS \"~._) \n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ---\n",
    "\n",
    "# (_.~\" METHODES \"~._)  \n",
    "\n",
    "# NETTOYAGE SIMPLE DES BALISES\n",
    "\n",
    "def simpleClean(toClean) :\n",
    "\n",
    "    # Instance de finalClean\n",
    "    finalClean = []\n",
    "    \n",
    "    # Nettoyage de chaque entitée de la liste\n",
    "    for thingToClean in toClean :\n",
    "        to_clean = re.compile('<.*?>')\n",
    "        toCleanStr = str(thingToClean)\n",
    "        toCleanStr = re.sub(to_clean, '',toCleanStr)\n",
    "        finalClean.append(toCleanStr)\n",
    "    \n",
    "    # Remplacement de l'ancienne liste par la liste clean\n",
    "    return finalClean\n",
    "\n",
    "# ---\n",
    "\n",
    "# (_.~\" DECLARATIONS VARIABLES \"~._) \n",
    "\n",
    "# Instance du DataFrame principal\n",
    "\n",
    "df = pd.DataFrame(columns=['Date','Hardware','Units','Softwares'])\n",
    "\n",
    "# Instance des listes pour le scrapping\n",
    "\n",
    "yearList = [\"2017\",\"2018\",\"2019\",\"2020\",\"2021\",\"2022\",\"2023\"]\n",
    "monthList = [\"02\",\"05\",\"08\",\"11\"]\n",
    "\n",
    "# Déclaration des variables nécessaires aux URLs où la place 02 et 04\n",
    "# sont réservés pour les dates et les hardwares\n",
    "\n",
    "url01 = \"https://web.archive.org/web/\"\n",
    "url03 = \"15120000/https://www.nintendo.co.jp/ir/en/finance/hard_soft/index.html\"\n",
    "\n",
    "# Variables nécessaire à la création d'URL\n",
    "\n",
    "urlList = []\n",
    "url = \"\"\n",
    "\n",
    "# ---\n",
    "\n",
    "# (_.~\" CODE \"~._)  \n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"_ - _ SCRAPPING _ - _\")\n",
    "print(\"\\n\")\n",
    "print(\"Boucle en cour :\")\n",
    "\n",
    "for year in yearList:\n",
    "        \n",
    "    # Pour savoir où on se situe en scrapping\n",
    "    print(year)\n",
    "        \n",
    "    for month in monthList:\n",
    "        url = url01 + year + month + url03\n",
    "            \n",
    "        # (_.~\" SCRAPPIIIIING TIME \"~._)\n",
    "   \n",
    "        # Récupération et stockage du code HTML\n",
    "        page = urlopen(url)\n",
    "        soup = bs(page, \"html.parser\")\n",
    "            \n",
    "        # ---\n",
    "    \n",
    "        # oOoOoOo TEXT MINING DES DONNEES oOoOoOo\n",
    "    \n",
    "        # Récupération de la date de mis à Jour des données selon Nintendo\n",
    "        date = soup.find('div', attrs = {'class' : 'sinf_date'}).string\n",
    "    \n",
    "        # Recupération de l'année et du mois de la mise à jour\n",
    "        dateUpdateStr = date[8:-2]\n",
    "        dateUpdate = datetime.strptime(dateUpdateStr, '%B %d, %Y')\n",
    "\n",
    "        # Instance des listes contenant les titres et les ventes\n",
    "        hardwares = []\n",
    "        allSales = []\n",
    "        hardSales = []\n",
    "        softSales = []\n",
    "\n",
    "        # Récupération des données\n",
    "        hardwares = soup.findAll('p', attrs = {'class' : \"hard_name\"})\n",
    "        allSales = soup.findAll('span', attrs = {'class' : \"num\"})\n",
    "        # Nettoyage des données\n",
    "        hardwares = simpleClean(hardwares)\n",
    "        allSales = simpleClean(allSales)\n",
    "        \n",
    "        switch = True\n",
    "        \n",
    "        for number in allSales:\n",
    "            if switch == True:\n",
    "                hardSales.append(float(number))\n",
    "                switch = False\n",
    "            else:\n",
    "                softSales.append(float(number))\n",
    "                switch = True\n",
    "        \n",
    "        # ---\n",
    "    \n",
    "        # oOoOoOo CREATION DU DATAFRAME oOoOoOo\n",
    "            \n",
    "        # Création des données à insérer dans les colonnes Date et Console\n",
    "        tempDateList = [dateUpdate for i in range(len(hardwares))]\n",
    "\n",
    "    \n",
    "        # Création du DataFrame contenant les lignes à ajouter\n",
    "        dfSoup = pd.DataFrame(list(zip(tempDateList, hardwares, hardSales, softSales)), \n",
    "                              columns=['Date','Hardware','Units','Softwares'])\n",
    "\n",
    "        # Fusion des deux DataFrame\n",
    "        df = pd.concat([df, dfSoup])\n",
    "            \n",
    "\n",
    "# ---\n",
    "\n",
    "print(\"Scrapping terminé !\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# (_.~\" NETTOYAGE PRIMAIRE DU DATAFRAME \"~._) \n",
    "\n",
    "print(\"_ - _ DATAFRAME _ - _\")\n",
    "print(\"\\n\")\n",
    "print(\"Check avant export :\")\n",
    "\n",
    "# Reset de l'index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Doublons check avant suppression\n",
    "doublons = df.duplicated().sum()\n",
    "if doublons > 0:\n",
    "    print(doublons,\" doublons présents dans le DataFrame.\")\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    doublons = df.duplicated().sum()\n",
    "    if doublons > 0:\n",
    "        print(doublons,\" doublons restent après suppression dans le DataFrame.\")\n",
    "    else:\n",
    "        print(\"Tous les doublons ont été supprimés !\")\n",
    "else:\n",
    "    print(\"Il n'y a aucun doublon dans le DataFrame.\")\n",
    "    \n",
    "# NaN check\n",
    "nanDfTotal = df.isna().sum().sum()\n",
    "nanDf = df.isna().sum()\n",
    "if nanDfTotal > 0:\n",
    "    print(\"Données manquantes dans le DataFrame\")\n",
    "    print(nanDf)\n",
    "else:\n",
    "    print(\"Il n'y a aucune données manquante dans le DataFrame.\")\n",
    "    \n",
    "# ---\n",
    "\n",
    "# (_.~\" EXPORT \"~._) \n",
    "\n",
    "# Export du Dataframe dans un fichier CSV\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Export en cours :\")\n",
    "\n",
    "csvName = 'nintendo_hardware_sales.csv'\n",
    "\n",
    "df.to_csv(csvName, index=False)\n",
    "print(\"Export réussi sous :\", csvName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
